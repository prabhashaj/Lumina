# Lumina AI Research Teaching Agent - Project Report

Date: 2026-02-17

## 1) Executive Summary

Lumina is a full-stack, multi-agent AI learning platform that combines live web research, structured teaching synthesis, and multiple learning modes in one experience. The system is built around a LangGraph-based orchestrator that coordinates specialized agents for intent classification, search, content extraction, image understanding, and teaching synthesis. On top of the research pipeline, the product adds exam preparation workflows, personalized learning paths, AI-generated video lectures with narration, and a doubt solver for image-based questions.

The platform is designed for pedagogical quality: it adapts difficulty, provides analogies and practice questions, includes source citations, and streams partial results to the UI for a responsive experience.

## 2) Goals and Scope

Primary goals
- Provide comprehensive, source-backed answers for student questions.
- Support multiple learning modes without changing tools or context.
- Stream partial results for fast feedback and better UX.
- Keep costs under control with search planning, caching, and model routing.

In scope
- Research mode with web search and visual understanding
- Exam prep roadmaps and quizzes
- Personalized learning assessments and plans
- Video lecture slide generation with narration
- Doubt solving from images
- Guide chatbot for contextual tutoring
- Flashcard generation and spaced repetition
- Code playground endpoints for execution and explanation

Out of scope (current version)
- Production-grade authentication and user accounts
- Full database persistence (uses localStorage on frontend)
- Continuous evaluation pipelines or A/B testing
- Production-grade sandboxing for code execution

## 3) System Architecture

### 3.1 High-level components

Backend
- FastAPI application in backend/main.py
- LangGraph orchestrator with agent nodes
- Search routing and caching
- SSE streaming endpoints for UI

Frontend
- Next.js (App Router) application
- Mode-based UI (research, exam prep, personalized, video lecture, doubt solver)
- Streaming content updates
- localStorage state management

Shared
- Pydantic schemas for consistent data models
- Centralized prompt templates

### 3.2 Multi-agent research flow

1) Intent Classifier
- Determines difficulty, question type, and learning needs.

2) Search Router
- Uses intent + heuristics to build a SearchPlan (depth, queries, results, raw content).
- Reduces cost and latency by tuning Tavily calls.

3) Web Search Agent (Tavily)
- Executes multi-query search, merges results, and aggregates images.

4) Content Extraction Agent
- Filters and extracts educational content from search results.

5) Image Understanding Agent
- Uses VLM or fallback to caption and rank images.

6) Teaching Synthesis Agent
- Generates teaching content with a strict structure.
- Creates TL;DR, step-by-step explanation, visual explanation, analogies, and questions.

7) Quality Assessment
- Evaluates response and triggers retry if needed.

### 3.3 Streaming UI

All major generation flows stream data via SSE. The UI renders partial results as they arrive, including:
- Status updates (progress text)
- TL;DR
- Explanation
- Images
- Sources
- Analogy
- Practice questions

This reduces perceived latency and gives real-time feedback during long operations.

## 4) Feature Details and Workflows

### 4.1 Research Mode

Workflow
1) User submits a question (optionally with image or file attachments).
2) Intent is classified to adapt depth and style.
3) Search Router decides depth and query count.
4) Tavily search returns sources and images.
5) Content extraction narrows to relevant educational text.
6) VLM analyzes images and ranks best visuals.
7) Teaching synthesis produces structured explanation.
8) Quality assessment gates output; retry if low.
9) SSE stream sends partial content to UI.

User outputs
- TL;DR summary
- Step-by-step explanation
- Visual explanations and images
- Real-world analogy
- Practice questions
- Sources and citations

### 4.2 Exam Prep Mode

Workflow
1) User enters a subject.
2) Backend generates a roadmap (5-8 chapters, 3-6 topics per chapter).
3) User selects a topic.
4) The topic is streamed using the research pipeline with an exam-prep prompt.
5) Quiz generation creates five MCQs with explanations.
6) Progress unlocks sequential topics and tracks quiz scores.

Persistence
- All session data is stored in localStorage.

### 4.3 Personalized Learning

Workflow
1) User takes a diagnostic assessment (6 questions).
2) Backend scores answers by difficulty and sub-topic.
3) Learner profile is generated by the LLM.
4) A phased learning plan is created (3-4 phases, 2-3 topics each).
5) Selecting a topic streams content tailored to level, weaknesses, and learning style.

Personalization features
- Knowledge level (beginner/intermediate/advanced)
- Learning style (visual/textual/example-driven/practice-heavy)
- Strength and weakness targeting
- Tailored prompts for each topic

### 4.4 Video Lecture Mode

Workflow
1) User chooses a topic, slide count, and difficulty.
2) Slide generator creates a deck with layouts, bullet points, and speaker notes.
3) Slide images are resolved via Tavily search.
4) Narration agent generates audio (ElevenLabs) or falls back to browser TTS.
5) Streaming endpoint returns slide-by-slide updates.

UI features
- Whiteboard-style slide viewer
- Narration player with transcript highlighting
- Auto-advance after narration

### 4.5 Doubt Solver Mode

Workflow
1) User uploads an image (notes, problem, textbook page).
2) VLM extracts content and provides a detailed description.
3) LLM generates an explanation, solves problems, and creates practice questions.
4) Chat mode supports follow-up questions with image context.

### 4.6 Guide Chatbot

Context-aware assistant for:
- Exam prep: study tactics, quick questions, formulas
- Personalized learning: guidance on plan topics and progress
- Video lectures: clarification of slide content

### 4.7 Flashcards

Workflow
1) User provides a topic or content.
2) Backend generates flashcards with difficulty scores.
3) Frontend stores decks and schedules reviews via SM-2 algorithm.

### 4.8 Code Playground

Capabilities
- Execute Python or JavaScript code in subprocesses.
- Explain, debug, optimize, or generate exercises via LLM.

Notes
- Should be further sandboxed for production.

## 5) Data and State Management

Frontend storage (localStorage)
- User auth session (mocked)
- Research chat history
- Exam prep sessions
- Personalized learning sessions
- Flashcard decks and SM-2 review metadata

Backend storage
- In-memory search cache (SearchCache)
- Log file output and cost tracking log

## 6) Model and Provider Routing

Agent model priority (most agents)
1) OpenRouter Free Router (if OPENROUTER_API_KEY)
2) Mistral API (if MISTRAL_API_KEY)
3) Groq (if GROQ_API_KEY)
4) OpenAI (if OPENAI_API_KEY)

Special cases
- Code AI tutor uses the same priority chain.
- Doubt solver optimized for OpenRouter free router first.
- Vision analysis uses Groq VLM, then Mistral, then OpenAI, then Replicate.

## 7) Search Strategy and Caching

Search Router
- Classifies query complexity using intent and heuristics.
- Builds a SearchPlan to control depth, query count, and raw content usage.

Search cache
- TTL-based in-memory cache to reduce repeated Tavily queries.
- Capped size to avoid growth.

## 8) Content Quality Controls

- Teaching synthesis uses strict structured prompts.
- Quality assessment agent evaluates response completeness and can retry.
- Practice question generation is deduplicated and normalized.

## 9) Observability and Cost Tracking

- Loguru-based logging to backend/logs/app.log
- Tavily cost summaries appended to logs.txt
- Cost details included in API responses when available

## 10) Security and Privacy

Current posture
- API keys stored in backend/.env
- No production auth or database layer
- User data stored in localStorage

Recommended for production
- Add real authentication (OAuth / email)
- Store user data in a secure database
- Add rate limiting and request validation
- Harden code execution sandbox

## 11) Configuration

Key environment variables
- TAVILY_API_KEY, OPENAI_API_KEY, MISTRAL_API_KEY, GROQ_API_KEY, OPENROUTER_API_KEY
- REPLICATE_API_TOKEN, ELEVENLABS_API_KEY
- API_HOST, API_PORT, CORS_ORIGINS
- LOG_LEVEL, LOG_FILE

See backend/.env.example for baseline settings.

## 12) Deployment

Backend
- Render configuration available in render.yaml
- Runs via Uvicorn with environment variables

Frontend
- Next.js app deployable to Vercel or any Node host

## 13) Testing

Backend
- pytest and pytest-asyncio configured

Frontend
- No automated tests yet; relies on manual QA

## 14) Known Limitations

- Redis and vector DB configuration exists but is not wired into the live pipeline.
- Auth is localStorage-only and not secure for production.
- Some LLM providers and model names may require updating over time.

## 15) Suggested Enhancements

- Add persistent storage for users and sessions
- Integrate Redis caching and vector retrieval
- Add evaluation harness and regression tests
- Harden code execution and file uploads
- Add analytics for learning outcomes and retention

## 16) Reference Files

- Backend entry: backend/main.py
- Orchestrator: backend/graph/orchestrator.py
- Agents: backend/agents/
- Frontend app: frontend/app/app/page.tsx
- Landing page: frontend/app/(landing)/page.tsx
- Shared schemas: shared/schemas/models.py
- Prompt templates: shared/prompts/templates.py
